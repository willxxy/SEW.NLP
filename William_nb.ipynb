{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "William_nb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lWXzDb_YAhIw"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vylq5TdU6qyO",
        "outputId": "041b733a-07f5-490e-b61d-498693874e5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers==2.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md2yM8PLxUkj",
        "outputId": "0d38ad0a-fb2a-4c33-944c-f72658eca043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.21.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.0.53)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.7.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.64.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (1.0.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.5 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (1.26.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.5->boto3->transformers==2.8.0) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.5->boto3->transformers==2.8.0) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.5->boto3->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQByxqDAMo0s",
        "outputId": "128dab11-348e-4a34-bf41-327bbf1f9e8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('drive/MyDrive/SEW.NLP')"
      ],
      "metadata": {
        "id": "iaVX_mfiCLaC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "{'question' : 'q1',\n",
        "  'answer' : 'a1',\n",
        "  'input_ids_q : ''i1',\n",
        "  'input_ids_a : 'ai' ,\n",
        "  attention mask : \n",
        "  }\n",
        "\n",
        "SCIBERT \n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upSlDpKXw9v1",
        "outputId": "b1082121-b7f7-48fd-aa50-185959a14062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# creating file path\n",
        "dbfile = 'ucimlrepo.db'\n",
        "# Create a SQL connection to our SQLite database\n",
        "con = sqlite3.connect(dbfile)\n",
        "\n",
        "# creating cursor\n",
        "cur = con.cursor()\n",
        "\n",
        "# reading all table names\n",
        "table_list = [a for a in cur.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")]\n",
        "# here is you table list\n",
        "print(table_list)\n",
        "\n",
        "# Be sure to close the connection\n",
        "con.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzBlOLKf8YUh",
        "outputId": "33cc91ff-86f7-46c7-9a80-c53e4df5994d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('attributes',), ('creators',), ('datasets_old_schema',), ('dataset_creators',), ('dataset_keywords',), ('dataset_papers',), ('descriptive_questions',), ('donated_datasets',), ('evals',), ('foreign_papers',), ('keywords',), ('metrics',), ('models',), ('native_papers',), ('slugs',), ('tabular',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('ucimlrepo.db')\n",
        "# cursor = conn.cursor()\n",
        "\n",
        "\n",
        "#%%\n",
        "#Select all columns and instances from a table\n",
        "query=\"SELECT * FROM donated_datasets\"\n",
        "result1=pd.read_sql(query, conn)\n",
        "\n",
        "#%%\n",
        "#Join\n",
        "query='''SELECT * \n",
        "        FROM donated_datasets a \n",
        "        LEFT JOIN descriptive_questions  b ON a.ID=b.datasetID'''\n",
        "\n",
        "result2=pd.read_sql(query, conn)\n",
        "\n",
        "# #%%\n",
        "# #Joins + Condition\n",
        "query='''\n",
        "        SELECT c.name as dataset_name, a.*, b.* \n",
        "        FROM dataset_keywords a \n",
        "        LEFT JOIN keywords  b ON a.keywordsID=b.id\n",
        "        LEFT JOIN donated_datasets c on a.datasetID=c.ID\n",
        "        WHERE c.name='Adult'\n",
        "        '''\n",
        "\n",
        "result3=pd.read_sql(query, conn)\n",
        "\n",
        "\n",
        "# #%%\n",
        "# #List all tables\n",
        "query='''\n",
        "\tSELECT name \n",
        "\tFROM sqlite_master \n",
        "\tWHERE type= \"table\"\n",
        "\t'''\n",
        "\n",
        "result4=pd.read_sql(query, conn)\n",
        "\n",
        "# #%%\n",
        "# #Count records\n",
        "query=\"SELECT count(*) from foreign_papers\"\n",
        "\n",
        "result5=pd.read_sql(query, conn)\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "tb8gzEUxCPa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result2"
      ],
      "metadata": {
        "id": "ku2ZlXiyFe3b",
        "outputId": "515ecfea-1ed6-498a-e81d-406003d3c252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID  userID  introPaperID  \\\n",
              "0      1       1           NaN   \n",
              "1      2       1           NaN   \n",
              "2      3       1           NaN   \n",
              "3      4       1           NaN   \n",
              "4      5       1           NaN   \n",
              "..   ...     ...           ...   \n",
              "597  696      19        2291.0   \n",
              "598  697     461        2292.0   \n",
              "599  713     620        2303.0   \n",
              "600  715     578        2305.0   \n",
              "601  722     867        2312.0   \n",
              "\n",
              "                                               Name  \\\n",
              "0                                           Abalone   \n",
              "1                                             Adult   \n",
              "2                                         Annealing   \n",
              "3                      Anonymous Microsoft Web Data   \n",
              "4                                        Arrhythmia   \n",
              "..                                              ...   \n",
              "597                            Open Web Text Corpus   \n",
              "598  Predict students' dropout and academic success   \n",
              "599                            Auction Verification   \n",
              "600           LT-FS-ID: Intrusion detection in WSNs   \n",
              "601      NATICUSdroid (Android Permissions) Dataset   \n",
              "\n",
              "                                              Abstract              Area  \\\n",
              "0    Predict the age of abalone from physical measu...              Life   \n",
              "1    Predict whether income exceeds $50K/yr based o...            Social   \n",
              "2                                 Steel annealing data          Physical   \n",
              "3    Log of anonymous users of www.microsoft.com; p...          Computer   \n",
              "4    Distinguish between the presence and absence o...              Life   \n",
              "..                                                 ...               ...   \n",
              "597  We started by extracting all Reddit post urls ...             Other   \n",
              "598  A dataset created from a higher education inst...             Other   \n",
              "599  We modeled a simultaneous multi-round auction ...  Computer Science   \n",
              "600  There exist five columns in this dataset. The ...  Computer Science   \n",
              "601  Contains permissions extracted from more than ...  Computer Science   \n",
              "\n",
              "                                       Task         Types  \\\n",
              "0                            Classification  Multivariate   \n",
              "1                            Classification  Multivariate   \n",
              "2                            Classification  Multivariate   \n",
              "3                       Recommender-Systems          None   \n",
              "4                            Classification  Multivariate   \n",
              "..                                      ...           ...   \n",
              "597  Classification, Regression, Clustering          Text   \n",
              "598                          Classification       Tabular   \n",
              "599              Classification, Regression       Tabular   \n",
              "600                              Regression       Tabular   \n",
              "601                          Classification       Tabular   \n",
              "\n",
              "                                   DOI DateDonated  ...  \\\n",
              "0                                 None  1995-12-01  ...   \n",
              "1                                 None  1996-05-01  ...   \n",
              "2                                 None        None  ...   \n",
              "3                                 None  1998-11-01  ...   \n",
              "4                                 None  1998-01-01  ...   \n",
              "..                                 ...         ...  ...   \n",
              "597                               None  2021-12-03  ...   \n",
              "598                                  1  2021-12-13  ...   \n",
              "599                               None  2022-03-01  ...   \n",
              "600  https://doi.org/10.3390/s22031070  2022-03-09  ...   \n",
              "601                               None  2022-04-25  ...   \n",
              "\n",
              "                                               purpose  \\\n",
              "0                                                 None   \n",
              "1                                                 None   \n",
              "2                                                 None   \n",
              "3                                                 None   \n",
              "4                                                 None   \n",
              "..                                                 ...   \n",
              "597                                               None   \n",
              "598  The dataset was created in a project that aims...   \n",
              "599  The dataset was created as part of a scientifi...   \n",
              "600  We generated this dataset for fast intrusion d...   \n",
              "601  This was for creating a Android malware detect...   \n",
              "\n",
              "                                               funding  \\\n",
              "0                                                 None   \n",
              "1                                                 None   \n",
              "2                                                 None   \n",
              "3                                                 None   \n",
              "4                                                 None   \n",
              "..                                                 ...   \n",
              "597                                               None   \n",
              "598  This dataset is supported by program SATDAP - ...   \n",
              "599                                               None   \n",
              "600                  Mentioned in the published paper.   \n",
              "601                                         University   \n",
              "\n",
              "                                             represent  \\\n",
              "0    Given is the attribute name, attribute type, t...   \n",
              "1    Listing of attributes:\\n\\n>50K, <=50K.\\n\\nage:...   \n",
              "2    Attribute Listing:\\n    1. family:\\t\\t--,GB,GK...   \n",
              "3    Each attribute is an area (\"vroot\") of the www...   \n",
              "4    -- Complete attribute documentation:\\n      1 ...   \n",
              "..                                                 ...   \n",
              "597                                               None   \n",
              "598                                               None   \n",
              "599  Each instance represents one verification run....   \n",
              "600                  Mentioned in the published paper.   \n",
              "601                presence of a permission in an app.   \n",
              "\n",
              "                                            dataSplits sensitiveInfo  \\\n",
              "0                                                 None          None   \n",
              "1                                                 None          None   \n",
              "2                                                 None          None   \n",
              "3                                                 None          None   \n",
              "4                                                 None          None   \n",
              "..                                                 ...           ...   \n",
              "597                                               None          None   \n",
              "598  The dataset was used, in out project, with a d...          None   \n",
              "599                                               None          None   \n",
              "600                  Mentioned in the published paper.          None   \n",
              "601                    70-30, 10-fold cross validation          None   \n",
              "\n",
              "                              preprocessingDescription  softwareAvailable  \\\n",
              "0                                                 None               None   \n",
              "1                                                 None               None   \n",
              "2                                                 None               None   \n",
              "3                                                 None               None   \n",
              "4                                                 None               None   \n",
              "..                                                 ...                ...   \n",
              "597                                               None               None   \n",
              "598  We performed a rigorous data preprocessing to ...               None   \n",
              "599                                               None               None   \n",
              "600             Well explained in the published paper.               None   \n",
              "601                                               None               None   \n",
              "\n",
              "                                                  used  \\\n",
              "0                                                 None   \n",
              "1                                                 None   \n",
              "2                                                 None   \n",
              "3                                                 None   \n",
              "4                                                 None   \n",
              "..                                                 ...   \n",
              "597                                               None   \n",
              "598  The dataset was used in a pilot project to pro...   \n",
              "599  The dataset was used for predictions of verifi...   \n",
              "600  For generating the LT-FS-ID algorithm. It has ...   \n",
              "601                                               None   \n",
              "\n",
              "                                             otherInfo  \\\n",
              "0    Predicting the age of abalone from physical me...   \n",
              "1    Extraction was done by Barry Becker from the 1...   \n",
              "2                                                 None   \n",
              "3    We created the data by sampling and processing...   \n",
              "4    This database contains 279 attributes, 206 of ...   \n",
              "..                                                 ...   \n",
              "597                                               None   \n",
              "598                                               None   \n",
              "599  Our code to prepare the dataset and to make pr...   \n",
              "600  For more details please visit;\\nhttps://www.ab...   \n",
              "601                                               None   \n",
              "\n",
              "                                       datasetCitation  \n",
              "0                                                 None  \n",
              "1                                                 None  \n",
              "2                                                 None  \n",
              "3                                                 None  \n",
              "4                                                 None  \n",
              "..                                                 ...  \n",
              "597                                               None  \n",
              "598  If you use this dataset in experiments for a s...  \n",
              "599  If you use this dataset in experiments for a s...  \n",
              "600  Singh, A.; Amutha, J.; Nagar, J.; Sharma, S.; ...  \n",
              "601  Mathur, A., Podila, L. M., Kulkarni, K., Niyaz...  \n",
              "\n",
              "[602 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7798ff43-49c3-4f29-ab05-44323d57f6ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>userID</th>\n",
              "      <th>introPaperID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Area</th>\n",
              "      <th>Task</th>\n",
              "      <th>Types</th>\n",
              "      <th>DOI</th>\n",
              "      <th>DateDonated</th>\n",
              "      <th>...</th>\n",
              "      <th>purpose</th>\n",
              "      <th>funding</th>\n",
              "      <th>represent</th>\n",
              "      <th>dataSplits</th>\n",
              "      <th>sensitiveInfo</th>\n",
              "      <th>preprocessingDescription</th>\n",
              "      <th>softwareAvailable</th>\n",
              "      <th>used</th>\n",
              "      <th>otherInfo</th>\n",
              "      <th>datasetCitation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Abalone</td>\n",
              "      <td>Predict the age of abalone from physical measu...</td>\n",
              "      <td>Life</td>\n",
              "      <td>Classification</td>\n",
              "      <td>Multivariate</td>\n",
              "      <td>None</td>\n",
              "      <td>1995-12-01</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Given is the attribute name, attribute type, t...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Predicting the age of abalone from physical me...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Adult</td>\n",
              "      <td>Predict whether income exceeds $50K/yr based o...</td>\n",
              "      <td>Social</td>\n",
              "      <td>Classification</td>\n",
              "      <td>Multivariate</td>\n",
              "      <td>None</td>\n",
              "      <td>1996-05-01</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Listing of attributes:\\n\\n&gt;50K, &lt;=50K.\\n\\nage:...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Extraction was done by Barry Becker from the 1...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Annealing</td>\n",
              "      <td>Steel annealing data</td>\n",
              "      <td>Physical</td>\n",
              "      <td>Classification</td>\n",
              "      <td>Multivariate</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Attribute Listing:\\n    1. family:\\t\\t--,GB,GK...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Anonymous Microsoft Web Data</td>\n",
              "      <td>Log of anonymous users of www.microsoft.com; p...</td>\n",
              "      <td>Computer</td>\n",
              "      <td>Recommender-Systems</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1998-11-01</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Each attribute is an area (\"vroot\") of the www...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>We created the data by sampling and processing...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Arrhythmia</td>\n",
              "      <td>Distinguish between the presence and absence o...</td>\n",
              "      <td>Life</td>\n",
              "      <td>Classification</td>\n",
              "      <td>Multivariate</td>\n",
              "      <td>None</td>\n",
              "      <td>1998-01-01</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>-- Complete attribute documentation:\\n      1 ...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>This database contains 279 attributes, 206 of ...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>696</td>\n",
              "      <td>19</td>\n",
              "      <td>2291.0</td>\n",
              "      <td>Open Web Text Corpus</td>\n",
              "      <td>We started by extracting all Reddit post urls ...</td>\n",
              "      <td>Other</td>\n",
              "      <td>Classification, Regression, Clustering</td>\n",
              "      <td>Text</td>\n",
              "      <td>None</td>\n",
              "      <td>2021-12-03</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>697</td>\n",
              "      <td>461</td>\n",
              "      <td>2292.0</td>\n",
              "      <td>Predict students' dropout and academic success</td>\n",
              "      <td>A dataset created from a higher education inst...</td>\n",
              "      <td>Other</td>\n",
              "      <td>Classification</td>\n",
              "      <td>Tabular</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-12-13</td>\n",
              "      <td>...</td>\n",
              "      <td>The dataset was created in a project that aims...</td>\n",
              "      <td>This dataset is supported by program SATDAP - ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The dataset was used, in out project, with a d...</td>\n",
              "      <td>None</td>\n",
              "      <td>We performed a rigorous data preprocessing to ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The dataset was used in a pilot project to pro...</td>\n",
              "      <td>None</td>\n",
              "      <td>If you use this dataset in experiments for a s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>713</td>\n",
              "      <td>620</td>\n",
              "      <td>2303.0</td>\n",
              "      <td>Auction Verification</td>\n",
              "      <td>We modeled a simultaneous multi-round auction ...</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Classification, Regression</td>\n",
              "      <td>Tabular</td>\n",
              "      <td>None</td>\n",
              "      <td>2022-03-01</td>\n",
              "      <td>...</td>\n",
              "      <td>The dataset was created as part of a scientifi...</td>\n",
              "      <td>None</td>\n",
              "      <td>Each instance represents one verification run....</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The dataset was used for predictions of verifi...</td>\n",
              "      <td>Our code to prepare the dataset and to make pr...</td>\n",
              "      <td>If you use this dataset in experiments for a s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>600</th>\n",
              "      <td>715</td>\n",
              "      <td>578</td>\n",
              "      <td>2305.0</td>\n",
              "      <td>LT-FS-ID: Intrusion detection in WSNs</td>\n",
              "      <td>There exist five columns in this dataset. The ...</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Regression</td>\n",
              "      <td>Tabular</td>\n",
              "      <td>https://doi.org/10.3390/s22031070</td>\n",
              "      <td>2022-03-09</td>\n",
              "      <td>...</td>\n",
              "      <td>We generated this dataset for fast intrusion d...</td>\n",
              "      <td>Mentioned in the published paper.</td>\n",
              "      <td>Mentioned in the published paper.</td>\n",
              "      <td>Mentioned in the published paper.</td>\n",
              "      <td>None</td>\n",
              "      <td>Well explained in the published paper.</td>\n",
              "      <td>None</td>\n",
              "      <td>For generating the LT-FS-ID algorithm. It has ...</td>\n",
              "      <td>For more details please visit;\\nhttps://www.ab...</td>\n",
              "      <td>Singh, A.; Amutha, J.; Nagar, J.; Sharma, S.; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>722</td>\n",
              "      <td>867</td>\n",
              "      <td>2312.0</td>\n",
              "      <td>NATICUSdroid (Android Permissions) Dataset</td>\n",
              "      <td>Contains permissions extracted from more than ...</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Classification</td>\n",
              "      <td>Tabular</td>\n",
              "      <td>None</td>\n",
              "      <td>2022-04-25</td>\n",
              "      <td>...</td>\n",
              "      <td>This was for creating a Android malware detect...</td>\n",
              "      <td>University</td>\n",
              "      <td>presence of a permission in an app.</td>\n",
              "      <td>70-30, 10-fold cross validation</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Mathur, A., Podila, L. M., Kulkarni, K., Niyaz...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>602 rows × 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7798ff43-49c3-4f29-ab05-44323d57f6ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7798ff43-49c3-4f29-ab05-44323d57f6ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7798ff43-49c3-4f29-ab05-44323d57f6ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in (os.listdir('csv')):\n",
        "  read = pd.read_csv(f'csv/{i}')\n",
        "  print(i)\n",
        "  print(len(read))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtsXRW7tDoE8",
        "outputId": "a3ac7181-4a37-4318-8677-76bf05a5c864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attributes.csv\n",
            "21544\n",
            "dataset_keywords.csv\n",
            "61\n",
            "dataset_creators.csv\n",
            "713\n",
            "creators.csv\n",
            "849\n",
            "slugs.csv\n",
            "611\n",
            "descriptive_questions.csv\n",
            "504\n",
            "keywords.csv\n",
            "98\n",
            "donated_datasets.csv\n",
            "602\n",
            "metrics.csv\n",
            "4\n",
            "evals.csv\n",
            "414\n",
            "tabular.csv\n",
            "596\n",
            "dataset_papers.csv\n",
            "26133\n",
            "models.csv\n",
            "10\n",
            "native_papers.csv\n",
            "126\n",
            "datasets_old_schema.csv\n",
            "491\n",
            "foreign_papers.csv\n",
            "11526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "iwzfMljgD13a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5074a3-52f2-40c7-9e94-6ca5d5c19c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv\t\t   qasper-train-dev-v0.3  ucimlrepo.db\n",
            "edoardo_eda.ipynb  sample_queries_NLP.py  William_nb.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "foreign = pd.read_csv('csv/foreign_papers.csv')\n",
        "native = pd.read_csv('csv/native_papers.csv')\n",
        "desc  = pd.read_csv('csv/descriptive_questions.csv')\n",
        "donated = pd.read_csv('csv/donated_datasets.csv')"
      ],
      "metadata": {
        "id": "ub4hL6BOVYmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foreign"
      ],
      "metadata": {
        "id": "UXlDvWxZvfZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foreign = foreign['DOI'].dropna()\n",
        "native = native['DOI'].dropna()\n",
        "donated = donated['DOI'].dropna()"
      ],
      "metadata": {
        "id": "0YraIYMFR1Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foreign = pd.DataFrame(foreign)\n",
        "native = pd.DataFrame(native)\n",
        "\n",
        "df = pd.concat([foreign, native], axis = 0)"
      ],
      "metadata": {
        "id": "WhSL4q-4SfXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('df.txt', 'a') as f:\n",
        "    dfAsString = df.to_string(header=False, index=False)\n",
        "    f.write(dfAsString)"
      ],
      "metadata": {
        "id": "uUvp5PkQP4HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_duplicate_papers(df, id_columns):\n",
        "  dropped_indices = pd.Index([])\n",
        "\n",
        "  for column in id_columns:\n",
        "    to_drop = df.index[(df[column].duplicated()) & ~df[column].isna()]\n",
        "    dropped_indices = dropped_indices.union(to_drop)\n",
        "\n",
        "  return df.drop(dropped_indices)"
      ],
      "metadata": {
        "id": "qHC8d2VsuiH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foreign_papers = drop_duplicate_papers(foreign, ['DOI', 'abstract', 'arxiv_id', 'acl_id', 'pmc_id', 'pubmed_id'])"
      ],
      "metadata": {
        "id": "-NMHVJsRu18j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "native_papers = drop_duplicate_papers(native, ['DOI', 'URL', 'arxiv', 'mag', 'acl', 'pmid', 'pmcid'])"
      ],
      "metadata": {
        "id": "oR2GAaqMvIvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foreign_papers = pd.DataFrame(foreign_papers)\n",
        "native_papers = pd.DataFrame(native_papers)"
      ],
      "metadata": {
        "id": "PkmINLa2yfKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foreign = foreign_papers['DOI']\n",
        "native = native_papers['DOI']"
      ],
      "metadata": {
        "id": "FhLyH4gpyro4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foreign"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHvjR385yzsX",
        "outputId": "5ecd5c16-d834-49f0-c2fb-489308845ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0            10.1109/ACCESS.2018.2860791\n",
              "1            10.1016/j.procs.2010.12.048\n",
              "2                     10.5120/13253-0729\n",
              "3          10.23919/EUSIPCO.2017.8081217\n",
              "4                                    NaN\n",
              "                      ...               \n",
              "11520    10.1152/japplphysiol.00887.2006\n",
              "11521               10.1155/2016/8048246\n",
              "11522       10.1016/j.patcog.2007.10.009\n",
              "11523            10.24002/ijis.v2i1.2352\n",
              "11525           10.3329/bjsr.v28i1.26238\n",
              "Name: DOI, Length: 10990, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "foreign = foreign_papers['DOI'].dropna()\n",
        "native = native_papers['DOI'].dropna()"
      ],
      "metadata": {
        "id": "1FaEzEsnvWHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdf = pd.DataFrame(foreign)"
      ],
      "metadata": {
        "id": "BR3sslrlx8JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf = pd.DataFrame(native)"
      ],
      "metadata": {
        "id": "8Zurb88Ly76j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.concat([fdf, ndf], axis =0)"
      ],
      "metadata": {
        "id": "1V_OyIDnxbsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vJHOQqeRxTqX",
        "outputId": "e42d31f6-7aba-4b1c-ff79-30675427f8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               DOI\n",
              "0                      10.1109/ACCESS.2018.2860791\n",
              "1                      10.1016/j.procs.2010.12.048\n",
              "2                               10.5120/13253-0729\n",
              "3                    10.23919/EUSIPCO.2017.8081217\n",
              "5                      10.1016/j.procs.2013.05.169\n",
              "..                                             ...\n",
              "99      https://doi.org/10.1038/s41598-021-97962-5\n",
              "104                 10.1021/acsfoodscitech.1c00320\n",
              "106              https://doi.org/10.3390/s22031070\n",
              "108  https://doi.org/10.1080/08839514.2021.2008612\n",
              "109                    10.1109/ACCESS.2022.3154445\n",
              "\n",
              "[9461 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17fdad15-8a62-4b55-8364-dc48ab586abc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DOI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.1109/ACCESS.2018.2860791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.1016/j.procs.2010.12.048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.5120/13253-0729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.23919/EUSIPCO.2017.8081217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10.1016/j.procs.2013.05.169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>https://doi.org/10.1038/s41598-021-97962-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>10.1021/acsfoodscitech.1c00320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>https://doi.org/10.3390/s22031070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>https://doi.org/10.1080/08839514.2021.2008612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>10.1109/ACCESS.2022.3154445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9461 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17fdad15-8a62-4b55-8364-dc48ab586abc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17fdad15-8a62-4b55-8364-dc48ab586abc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17fdad15-8a62-4b55-8364-dc48ab586abc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "abstract\n",
        "introduction\n",
        "dataset\n",
        "references \n",
        "literature review\n",
        "result \n",
        "title\n",
        "authors\n",
        "year\n",
        "'''\n",
        "\n",
        "'''\n",
        "{\n",
        "'Descriptive_Questions' :\n",
        "\t{\n",
        "\t'Question1' : 'For what purpose was the dataset created?',\n",
        "\t'Question2' : 'Who funded the creation of the dataset?',\n",
        "\t'Question3' : 'What do the instances that comprise the dataset represent?',\n",
        "\t'Question4' : 'Are there recommended data splits?',\n",
        "\t'Question5' : 'Does the dataset contain data that might be considered sensitive in any way?',\n",
        "\t'Question6' : 'Was there any data preprocessing performed?'\n",
        "\t},\n",
        "'Features' : ['attribute_name', 'role', 'type', \n",
        "\t\t'description', 'units', 'missing_values']\n",
        "\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "AU5giAUq_DcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vR5JK6v1Dtj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser\n",
        "\n",
        "webbrowser.open('https://sci-hub.hkvisa.net/') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k05QS_YHJzmb",
        "outputId": "e2f9e89c-acb4-4197-fff4-92302f02f254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPaperBot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nPxBjsDJ2C5",
        "outputId": "93757dba-d1b4-47a3-87d9-bdc2336dacc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPaperBot\n",
            "  Downloading PyPaperBot-1.2.2-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: wrapt>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (1.14.1)\n",
            "Collecting astroid<=2.5,>=2.4.2\n",
            "  Downloading astroid-2.5-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 31.4 MB/s \n",
            "\u001b[?25hCollecting crossref-commons>=0.0.7\n",
            "  Downloading crossref_commons-0.0.7-py3-none-any.whl (14 kB)\n",
            "Collecting urllib3>=1.25.10\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 76.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (1.3.5)\n",
            "Collecting bibtexparser>=1.2.0\n",
            "  Downloading bibtexparser-1.2.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy>=1.4.3\n",
            "  Downloading lazy_object_proxy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting mccabe>=0.6.1\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting HTMLParser>=0.0.2\n",
            "  Downloading HTMLParser-0.0.2.tar.gz (6.0 kB)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (3.0.4)\n",
            "Collecting colorama>=0.4.3\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting ratelimit>=2.2.1\n",
            "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
            "Collecting future>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 60.1 MB/s \n",
            "\u001b[?25hCollecting toml>=0.10.1\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (1.15.0)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.10 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (2.10)\n",
            "Collecting beautifulsoup4>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 67.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soupsieve>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (2.3.2.post1)\n",
            "Collecting pyChainedProxy>=1.1\n",
            "  Downloading pyChainedProxy-1.2.tar.gz (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (2.8.2)\n",
            "Collecting pylint>=2.6.0\n",
            "  Downloading pylint-2.13.9-py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 45.9 MB/s \n",
            "\u001b[?25hCollecting isort>=5.4.2\n",
            "  Downloading isort-5.10.1-py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 62.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2020.6.20 in /usr/local/lib/python3.7/dist-packages (from PyPaperBot) (2021.10.8)\n",
            "Collecting wrapt>=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting typed-ast<1.5,>=1.4.0\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 38.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.6.0->PyPaperBot) (4.2.0)\n",
            "Requirement already satisfied: dill>=0.2 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.6.0->PyPaperBot) (0.3.4)\n",
            "Collecting platformdirs>=2.2.0\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.6.0->PyPaperBot) (2.0.1)\n",
            "Collecting pylint>=2.6.0\n",
            "  Downloading pylint-2.13.8-py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 47.3 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.13.7-py3-none-any.whl (437 kB)\n",
            "\u001b[K     |████████████████████████████████| 437 kB 11.1 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.13.6-py3-none-any.whl (437 kB)\n",
            "\u001b[K     |████████████████████████████████| 437 kB 49.3 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.13.5-py3-none-any.whl (437 kB)\n",
            "\u001b[K     |████████████████████████████████| 437 kB 46.1 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.13.4-py3-none-any.whl (437 kB)\n",
            "\u001b[K     |████████████████████████████████| 437 kB 51.5 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.13.3-py3-none-any.whl (437 kB)\n",
            "\u001b[K     |████████████████████████████████| 437 kB 44.0 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.13.2-py3-none-any.whl (437 kB)\n",
            "\u001b[K     |████████████████████████████████| 437 kB 43.6 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.13.1-py3-none-any.whl (436 kB)\n",
            "\u001b[K     |████████████████████████████████| 436 kB 39.2 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.13.0-py3-none-any.whl (436 kB)\n",
            "\u001b[K     |████████████████████████████████| 436 kB 37.1 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.12.2-py3-none-any.whl (414 kB)\n",
            "\u001b[K     |████████████████████████████████| 414 kB 36.6 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.12.1-py3-none-any.whl (413 kB)\n",
            "\u001b[K     |████████████████████████████████| 413 kB 47.2 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.12.0-py3-none-any.whl (413 kB)\n",
            "\u001b[K     |████████████████████████████████| 413 kB 8.0 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.11.1-py3-none-any.whl (392 kB)\n",
            "\u001b[K     |████████████████████████████████| 392 kB 43.7 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.11.0-py3-none-any.whl (391 kB)\n",
            "\u001b[K     |████████████████████████████████| 391 kB 54.1 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.10.2-py3-none-any.whl (392 kB)\n",
            "\u001b[K     |████████████████████████████████| 392 kB 49.1 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.10.1-py3-none-any.whl (392 kB)\n",
            "\u001b[K     |████████████████████████████████| 392 kB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.6.0->PyPaperBot) (1.4.4)\n",
            "  Downloading pylint-2.10.0-py3-none-any.whl (392 kB)\n",
            "\u001b[K     |████████████████████████████████| 392 kB 11.2 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.9.6-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 54.3 MB/s \n",
            "\u001b[?25hCollecting mccabe>=0.6.1\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting pylint>=2.6.0\n",
            "  Downloading pylint-2.9.5-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 73.2 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.9.4-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 61.7 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.9.3-py3-none-any.whl (372 kB)\n",
            "\u001b[K     |████████████████████████████████| 372 kB 63.8 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.9.2-py3-none-any.whl (371 kB)\n",
            "\u001b[K     |████████████████████████████████| 371 kB 58.3 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.9.1-py3-none-any.whl (371 kB)\n",
            "\u001b[K     |████████████████████████████████| 371 kB 54.1 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.9.0-py3-none-any.whl (371 kB)\n",
            "\u001b[K     |████████████████████████████████| 371 kB 66.2 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.8.3-py3-none-any.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 75.4 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.8.2-py3-none-any.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 68.8 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.8.1-py3-none-any.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 62.5 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.8.0-py3-none-any.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 64.8 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.7.4-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 64.8 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.7.3-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 59.6 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.7.2-py3-none-any.whl (342 kB)\n",
            "\u001b[K     |████████████████████████████████| 342 kB 58.8 MB/s \n",
            "\u001b[?25h  Downloading pylint-2.7.1-py3-none-any.whl (343 kB)\n",
            "\u001b[K     |████████████████████████████████| 343 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->PyPaperBot) (2.0.12)\n",
            "Building wheels for collected packages: bibtexparser, future, HTMLParser, pyChainedProxy, ratelimit, wrapt\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.2.0-py3-none-any.whl size=36713 sha256=19cefed89bc7a88fd95fa2133555df75867aa838b29701fb01710e05799a7c95\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/13/1d/09c37a40f39ddd7b226719a797f1896a5b95d730de27e7a505\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=da902635f19ebaab7b94dd7086a555a9a30506875b3ca8eb6df3da5d3cb029c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for HTMLParser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for HTMLParser: filename=HTMLParser-0.0.2-py3-none-any.whl size=5983 sha256=b5e7126874e6f625c13312225f1ea6e03b0a7e2687a497876ae10130d53aae3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/0f/43/11747d95b28379b346c15f935f4d4075e7a4ec068d3a510c79\n",
            "  Building wheel for pyChainedProxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyChainedProxy: filename=pyChainedProxy-1.2-py3-none-any.whl size=16064 sha256=8a19d3615fffc5f2a168ed71b7cd42f4277b44861d3adc2887ff851312c844bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/f0/d7/05d060d5e66a373b435f0cd91b67eac62d884040d6f5a8db8a\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5908 sha256=c4b7d53b5864614fcf0f081625bafad2a85a571ee6d60ec926a30ced746e859e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/c2/23/4915cca200175fece0d5015f1981f4e1ecb5e3ef40b66cf525\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68724 sha256=20cb11bb740e91fabbd4314cb1757b7519666cb9b8aa8cd29de4e5c9838b914e\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built bibtexparser future HTMLParser pyChainedProxy ratelimit wrapt\n",
            "Installing collected packages: wrapt, urllib3, typed-ast, lazy-object-proxy, toml, requests, ratelimit, mccabe, isort, future, astroid, pylint, pyChainedProxy, HTMLParser, crossref-commons, colorama, bibtexparser, beautifulsoup4, PyPaperBot\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed HTMLParser-0.0.2 PyPaperBot-1.2.2 astroid-2.5 beautifulsoup4-4.11.1 bibtexparser-1.2.0 colorama-0.4.4 crossref-commons-0.0.7 future-0.18.2 isort-5.10.1 lazy-object-proxy-1.7.1 mccabe-0.6.1 pyChainedProxy-1.2 pylint-2.7.1 ratelimit-2.2.1 requests-2.27.1 toml-0.10.2 typed-ast-1.4.3 urllib3-1.26.9 wrapt-1.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "read = pd.read_csv('paper_dataset.csv')"
      ],
      "metadata": {
        "id": "3GoHk8smNB2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read = read[['ID', 'Name', ]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TlbMe3-zTS-j",
        "outputId": "f5faa767-9b84-48e6-8150-ca0a3ed855e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0   ID                                      Name      Area  \\\n",
              "0             0    1                                   Abalone      Life   \n",
              "1             1    1                                   Abalone      Life   \n",
              "2           163    2                                     Adult    Social   \n",
              "3           164    2                                     Adult    Social   \n",
              "4           896    3                                 Annealing  Physical   \n",
              "..          ...  ...                                       ...       ...   \n",
              "479       16292  471  Electrical Grid Stability Simulated Data  Physical   \n",
              "480       16293  471  Electrical Grid Stability Simulated Data  Physical   \n",
              "481       16294  492           Metro Interstate Traffic Volume     Other   \n",
              "482       16295  500                                       MEx  Computer   \n",
              "483       16296  500                                       MEx  Computer   \n",
              "\n",
              "                                                 title    year  \\\n",
              "0    SUPPORT VECTOR REGRESSION WITH A GENERALIZED Q...  2004.0   \n",
              "1    Classifier selection based on data complexity ...  2005.0   \n",
              "2    Are Hybrid Fibers a Common Motif of Canine Lar...  2000.0   \n",
              "3    Cell type specific tracing of the subcortical ...  2019.0   \n",
              "4    Global Induction of Decision Trees: From Paral...  2008.0   \n",
              "..                                                 ...     ...   \n",
              "479  Regression Analysis of Grid Stability under De...  2019.0   \n",
              "480  New Appliance Detection for Nonintrusive Load ...  2019.0   \n",
              "481  Sequence-to-Sequence Imputation of Missing Sen...  2020.0   \n",
              "482  Positional information in axolotl and mouse li...  2015.0   \n",
              "483  Pregnancy in a Jehovah's Witness with Cervical...  1998.0   \n",
              "\n",
              "                              DOI  rn  \n",
              "0        10.1007/1-4020-3432-6_25   1  \n",
              "1             10.1007/11578079_61   2  \n",
              "2      10.1001/archotol.126.7.865   1  \n",
              "3               10.1002/cne.24412   2  \n",
              "4    10.1007/978-3-540-69731-2_42   1  \n",
              "..                            ...  ..  \n",
              "479    10.1109/ICESI.2019.8863027   1  \n",
              "480      10.1109/TII.2019.2916213   2  \n",
              "481  10.1007/978-3-030-35288-2_22   1  \n",
              "482               10.1002/reg2.40   1  \n",
              "483        10.1006/gyno.1998.5187   2  \n",
              "\n",
              "[484 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7736224a-afa4-4619-b90e-52e62b6d4d32\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Area</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>DOI</th>\n",
              "      <th>rn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Abalone</td>\n",
              "      <td>Life</td>\n",
              "      <td>SUPPORT VECTOR REGRESSION WITH A GENERALIZED Q...</td>\n",
              "      <td>2004.0</td>\n",
              "      <td>10.1007/1-4020-3432-6_25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Abalone</td>\n",
              "      <td>Life</td>\n",
              "      <td>Classifier selection based on data complexity ...</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>10.1007/11578079_61</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>163</td>\n",
              "      <td>2</td>\n",
              "      <td>Adult</td>\n",
              "      <td>Social</td>\n",
              "      <td>Are Hybrid Fibers a Common Motif of Canine Lar...</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>10.1001/archotol.126.7.865</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>164</td>\n",
              "      <td>2</td>\n",
              "      <td>Adult</td>\n",
              "      <td>Social</td>\n",
              "      <td>Cell type specific tracing of the subcortical ...</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>10.1002/cne.24412</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>3</td>\n",
              "      <td>Annealing</td>\n",
              "      <td>Physical</td>\n",
              "      <td>Global Induction of Decision Trees: From Paral...</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>10.1007/978-3-540-69731-2_42</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>16292</td>\n",
              "      <td>471</td>\n",
              "      <td>Electrical Grid Stability Simulated Data</td>\n",
              "      <td>Physical</td>\n",
              "      <td>Regression Analysis of Grid Stability under De...</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>10.1109/ICESI.2019.8863027</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>16293</td>\n",
              "      <td>471</td>\n",
              "      <td>Electrical Grid Stability Simulated Data</td>\n",
              "      <td>Physical</td>\n",
              "      <td>New Appliance Detection for Nonintrusive Load ...</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>10.1109/TII.2019.2916213</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>16294</td>\n",
              "      <td>492</td>\n",
              "      <td>Metro Interstate Traffic Volume</td>\n",
              "      <td>Other</td>\n",
              "      <td>Sequence-to-Sequence Imputation of Missing Sen...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>10.1007/978-3-030-35288-2_22</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>16295</td>\n",
              "      <td>500</td>\n",
              "      <td>MEx</td>\n",
              "      <td>Computer</td>\n",
              "      <td>Positional information in axolotl and mouse li...</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>10.1002/reg2.40</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>16296</td>\n",
              "      <td>500</td>\n",
              "      <td>MEx</td>\n",
              "      <td>Computer</td>\n",
              "      <td>Pregnancy in a Jehovah's Witness with Cervical...</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>10.1006/gyno.1998.5187</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>484 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7736224a-afa4-4619-b90e-52e62b6d4d32')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7736224a-afa4-4619-b90e-52e62b6d4d32 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7736224a-afa4-4619-b90e-52e62b6d4d32');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m PyPaperBot --doi-file=\"df_9900.txt\" --dwn-dir=\"pdf\"\n"
      ],
      "metadata": {
        "id": "_Eh-rKwHNzDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('df.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# remove spaces\n",
        "lines = [line.replace(' ', '') for line in lines]\n",
        "\n",
        "# finally, write lines in the file\n",
        "with open('df.txt', 'w') as f:\n",
        "    f.writelines(lines)"
      ],
      "metadata": {
        "id": "EoN-tlnsMUrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_per_file = 900\n",
        "smallfile = None\n",
        "with open('df.txt') as bigfile:\n",
        "    for lineno, line in enumerate(bigfile):\n",
        "        if lineno % lines_per_file == 0:\n",
        "            if smallfile:\n",
        "                smallfile.close()\n",
        "            small_filename = 'df_{}.txt'.format(lineno + lines_per_file)\n",
        "            smallfile = open(small_filename, \"w\")\n",
        "        smallfile.write(line)\n",
        "    if smallfile:\n",
        "        smallfile.close()"
      ],
      "metadata": {
        "id": "Qvy1OhHm00IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "R2pTsZ8p2g6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02aeeed3-25ba-4d26-b4e1-142a41a014e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Copy of Copy of William_nb.ipynb'   df_6300.txt        edoardo_eda.ipynb\n",
            "'Copy of William_nb.ipynb'\t     df_7200.txt        pdf\n",
            " csv\t\t\t\t     df_8100.txt        PyPaperBot\n",
            " df_1800.txt\t\t\t     df_9000.txt        qasper-train-dev-v0.3\n",
            " df_2700.txt\t\t\t     df_900.txt         sample_queries_NLP.py\n",
            " df_3600.txt\t\t\t     df_9900.txt        ucimlrepo.db\n",
            " df_4500.txt\t\t\t     df_subsample.txt   William_nb.ipynb\n",
            " df_5400.txt\t\t\t     df.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('pdf'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zDBYZgx88T3",
        "outputId": "c9b052da-ad25-4e46-82c2-330b1500a984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7601"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir('pdf'):\n",
        "  if '.pdf' not in i:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "PQcQIlDr8_Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir('pdf'):\n",
        "  if '_Accuracy and artifact_ Reexamining the intensity bias in affective forecasting__ Correction to Levine et al. _2012_.' in i:\n",
        "    print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubw_G5oW2f1H",
        "outputId": "cd1269c1-34c0-4482-b985-51d5a421238d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_Accuracy and artifact_ Reexamining the intensity bias in affective forecasting__ Correction to Levine et al. _2012_..pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir('new_pdfs'):\n",
        "  if i == 'bibtex.bib' or i == 'result.gsheet':\n",
        "    os.remove(f'new_pdfs/{i}')"
      ],
      "metadata": {
        "id": "8RDDnB8D2sfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir('new_pdfs'):\n",
        "  if '.pdf' not in i:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEZOQj_2SHFR",
        "outputId": "da7df75d-f3cb-4afe-862f-fe490f32c43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JQBz2ueS5HP",
        "outputId": "f27af7db-df3b-4f04-e5f2-3f85b323461a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv\n",
            "data_retrieval.ipynb\n",
            "edoardo_eda.ipynb\n",
            "new_pdfs\n",
            "output_dir\n",
            "paper_dataset.csv\n",
            "paper_dataset.gsheet\n",
            "pdf\n",
            "PyPaperBot\n",
            "qasper-train-dev-v0.3\n",
            "result.csv\n",
            "result.gsheet\n",
            "sample_queries_NLP.py\n",
            "Short9organic9carbon9turnover9time9and9narrow_999999999999_sup_14__sup__999999999999C9age9spectra9in9early9Holocene9wetland9paleosols.json\n",
            "txt\n",
            "ucimlrepo.db\n",
            "William_nb.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "f = open('output_dir/Neural9Random9Forests.json')\n",
        "j = json.load(f)"
      ],
      "metadata": {
        "id": "Myfk_QBDdw85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in j:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1_Im5zJjnRL",
        "outputId": "43de45dd-4bcc-4353-cfc8-7c87bcaa0dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paper_id\n",
            "header\n",
            "title\n",
            "authors\n",
            "year\n",
            "venue\n",
            "identifiers\n",
            "abstract\n",
            "pdf_parse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in j['pdf_parse']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOg98pzQjyGa",
        "outputId": "3e618d32-55c8-46ae-d0bf-ec4a2df9aa63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paper_id\n",
            "_pdf_hash\n",
            "abstract\n",
            "body_text\n",
            "back_matter\n",
            "bib_entries\n",
            "ref_entries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "j['pdf_parse']['ref_entries']"
      ],
      "metadata": {
        "id": "fIX-RvO7kOjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(j['paper_id'])\n",
        "print(j['abstract'])\n",
        "for i in j['pdf_parse']['body_text']:\n",
        "  print(i['section'])\n",
        "  print(i['text'])\n",
        "print(j['pdf_parse']['ref_entries'])"
      ],
      "metadata": {
        "id": "hlQAmkvHlqKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "We tested the two regression methods on two datasets: the Abalone dataset from the UCI repository and a QSPR problem involving alkanes, i.e. chemical compounds represented as trees. For both datasets we report the results obtained by SVR and QLSVR. We employed a modified version of SVMLight 5.0 [Joachims, 1998] enabled to work with a kernel matrix generated by Scilab 2.7 c INRIA-ENPC.\n"
      ],
      "metadata": {
        "id": "eJrc38t04C2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in j['abstract']:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "gOMYJkGgd9bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-IsTJoCj47O",
        "outputId": "5c1d4af2-e637-484b-97c4-a48a292c4881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'question', 'answer', 'context', 'start_index',\n",
              "       'end_index', 'narrowed_context', 'start-end'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df = pd.read_csv('qasper_df.csv')\n",
        "X = df[['question', 'context']]\n",
        "y = df[['start_index', 'end_index', 'answer']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)\n",
        "\n",
        "df_train = pd.concat([X_train, y_train], axis = 1)\n",
        "df_val = pd.concat([X_val, y_val], axis = 1)\n",
        "df_test = pd.concat([X_test, y_test], axis = 1)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ixa-ehu/SciBERT-SQuAD-QuAC\")\n"
      ],
      "metadata": {
        "id": "t6ZqEAs05iRo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_train), len(df_val), len(df_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR-Ua7dalgRH",
        "outputId": "1b264dce-90b0-4043-ce83-2c576de58247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7187 898 899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def narrow_context(row):\n",
        "    context = row[\"context\"]\n",
        "    start_index = max(row[\"start_index\"] - 2000, 0) \n",
        "    end_index = max(row[\"end_index\"] + 2000, 4000)\n",
        "    end_index = min(end_index, len(context)-1)\n",
        "    return context[start_index:end_index]"
      ],
      "metadata": {
        "id": "peD2l0PVO6WD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"narrowed_context\"] = df_train.apply(narrow_context, axis=1)\n",
        "df_val[\"narrowed_context\"] = df_val.apply(narrow_context, axis=1)\n",
        "df_test[\"narrowed_context\"] = df_test.apply(narrow_context, axis=1)"
      ],
      "metadata": {
        "id": "mwLe7mLos65I"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_index(row):\n",
        "    answer = row[\"answer\"]\n",
        "    start_index = row[\"narrowed_context\"].find(answer) if row[\"start_index\"] != -1 else 0\n",
        "    end_index = start_index + len(answer) - 1 if row[\"start_index\"] != -1 else 0\n",
        "    return start_index, end_index\n",
        "\n",
        "\n",
        "df_train[\"start-end\"] = df_train.apply(find_index, axis=1)\n",
        "df_val[\"start-end\"] = df_val.apply(find_index, axis=1)\n",
        "df_test[\"start-end\"] = df_test.apply(find_index, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "search_for = [\"data\", \"feature\", \"variable\", \"result\", \"preprocessing\", \"labels\", \"baseline\"]\n",
        "\n",
        "df_train = df_train.loc[df_train[\"question\"].str.contains(\"|\".join(search_for))]\n",
        "df_val = df_val.loc[df_val[\"question\"].str.contains(\"|\".join(search_for))]\n",
        "df_test = df_test.loc[df_test[\"question\"].str.contains(\"|\".join(search_for))]"
      ],
      "metadata": {
        "id": "cSmOK-flP5zZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.reset_index()\n",
        "df_val = df_val.reset_index()\n",
        "df_test = df_test.reset_index()\n"
      ],
      "metadata": {
        "id": "GxWPEuifP7A_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"merged\"] = df_train['question'].astype(str) +\" \"+ df_train[\"narrowed_context\"]\n",
        "df_val[\"merged\"] = df_val['question'].astype(str) +\" \"+ df_val[\"narrowed_context\"]\n",
        "df_test[\"merged\"] = df_test['question'].astype(str) +\" \"+ df_test[\"narrowed_context\"]\n"
      ],
      "metadata": {
        "id": "65Yj-bR2S-q9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "stride = 256\n",
        "\n",
        "df_train_tokenized = tokenizer(\n",
        "    list(df_train['question']),\n",
        "    list(df_train['narrowed_context']),\n",
        "    max_length = MAX_LEN,\n",
        "    return_overflowing_tokens = True,\n",
        "    truncation = 'only_second',\n",
        "    return_offsets_mapping = True,\n",
        "    stride = stride,\n",
        "    padding = 'max_length'\n",
        ")\n",
        "\n",
        "df_val_tokenized = tokenizer(\n",
        "    list(df_val['question']),\n",
        "    list(df_val['narrowed_context']),\n",
        "    max_length = MAX_LEN,\n",
        "    return_overflowing_tokens = True,\n",
        "    truncation = 'only_second',\n",
        "    return_offsets_mapping = True,\n",
        "    stride = stride,\n",
        "    padding = 'max_length'\n",
        ")\n",
        "\n",
        "df_test_tokenized = tokenizer(\n",
        "    list(df_test['question']),\n",
        "    list(df_test['narrowed_context']),\n",
        "    max_length = MAX_LEN,\n",
        "    return_overflowing_tokens = True,\n",
        "    truncation = 'only_second',\n",
        "    return_offsets_mapping = True,\n",
        "    stride = stride,\n",
        "    padding = 'max_length'\n",
        ")\n"
      ],
      "metadata": {
        "id": "cVHkHaoNVTsi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_tokenized['input_ids'][0]"
      ],
      "metadata": {
        "id": "gJO8-6FZ3jm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offsets_mapping_train = df_train_tokenized.pop(\"offset_mapping\")\n",
        "offsets_mapping_val = df_val_tokenized.pop(\"offset_mapping\")\n",
        "offsets_mapping_test = df_test_tokenized.pop(\"offset_mapping\")"
      ],
      "metadata": {
        "id": "k4P4xwLVY4tO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers = df_train[\"answer\"]\n",
        "start_positions_train = []\n",
        "end_positions_train = []\n",
        "df_index = -1\n",
        "\n",
        "for i, offset in enumerate(offsets_mapping_train):\n",
        "    sequence_ids = df_train_tokenized.sequence_ids(i)\n",
        "    \n",
        "    # Find the start and end of the context\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "        idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "        idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    if offset[context_start][0] == 0:\n",
        "        df_index += 1\n",
        "    \n",
        "    start_char, end_char = df_train.loc[df_index, \"start-end\"]\n",
        "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "        start_positions_train.append(0)\n",
        "        end_positions_train.append(0)\n",
        "    else:\n",
        "        idx = context_start\n",
        "        while idx <= context_end and offset[idx][0] <= start_char:\n",
        "            idx += 1\n",
        "        start_positions_train.append(idx - 1)\n",
        "\n",
        "        idx = context_end\n",
        "        while idx >= context_start and offset[idx][1] >= end_char:\n",
        "            idx -= 1\n",
        "        end_positions_train.append(idx + 1)"
      ],
      "metadata": {
        "id": "3CP-DoFniMKP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers = df_val[\"answer\"]\n",
        "start_positions_val = []\n",
        "end_positions_val = []\n",
        "df_index = -1\n",
        "\n",
        "for i, offset in enumerate(offsets_mapping_val):\n",
        "    sequence_ids = df_val_tokenized.sequence_ids(i)\n",
        "    \n",
        "    # Find the start and end of the context\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "        idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "        idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    if offset[context_start][0] == 0:\n",
        "        df_index += 1\n",
        "    \n",
        "    start_char, end_char = df_val.loc[df_index, \"start-end\"]\n",
        "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "        start_positions_val.append(0)\n",
        "        end_positions_val.append(0)\n",
        "    else:\n",
        "        idx = context_start\n",
        "        while idx <= context_end and offset[idx][0] <= start_char:\n",
        "            idx += 1\n",
        "        start_positions_val.append(idx - 1)\n",
        "\n",
        "        idx = context_end\n",
        "        while idx >= context_start and offset[idx][1] >= end_char:\n",
        "            idx -= 1\n",
        "        end_positions_val.append(idx + 1)"
      ],
      "metadata": {
        "id": "qj9cQ1wjnJAI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers = df_test[\"answer\"]\n",
        "start_positions_test = []\n",
        "end_positions_test = []\n",
        "df_index = -1\n",
        "\n",
        "for i, offset in enumerate(offsets_mapping_test):\n",
        "    sequence_ids = df_test_tokenized.sequence_ids(i)\n",
        "    \n",
        "    # Find the start and end of the context\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "        idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "        idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    if offset[context_start][0] == 0:\n",
        "        df_index += 1\n",
        "    \n",
        "    start_char, end_char = df_test.loc[df_index, \"start-end\"]\n",
        "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "        start_positions_test.append(0)\n",
        "        end_positions_test.append(0)\n",
        "    else:\n",
        "        idx = context_start\n",
        "        while idx <= context_end and offset[idx][0] <= start_char:\n",
        "            idx += 1\n",
        "        start_positions_test.append(idx - 1)\n",
        "\n",
        "        idx = context_end\n",
        "        while idx >= context_start and offset[idx][1] >= end_char:\n",
        "            idx -= 1\n",
        "        end_positions_test.append(idx + 1)"
      ],
      "metadata": {
        "id": "i4AsDktWnJ1w"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_tokenized[\"start_positions\"] = start_positions_train\n",
        "df_train_tokenized[\"end_positions\"] = end_positions_train\n",
        "\n",
        "df_val_tokenized[\"start_positions\"] = start_positions_val\n",
        "df_val_tokenized[\"end_positions\"] = end_positions_val\n",
        "\n",
        "df_test_tokenized[\"start_positions\"] = start_positions_test\n",
        "df_test_tokenized[\"end_positions\"] = end_positions_test"
      ],
      "metadata": {
        "id": "frqhdarGiN0h"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIrljxR0jcGJ",
        "outputId": "eed75f53-6503-4e8f-bf1e-e90ca34b401c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 102, 1792,  220,  ...,  993,  147,  103],\n",
              "        [ 102, 1792,  220,  ...,  650,  195,  103],\n",
              "        [ 102, 1792,  220,  ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [ 102,  334, 6554,  ...,  205,  130,  103],\n",
              "        [ 102,  334, 6554,  ..., 1395, 2231,  103],\n",
              "        [ 102,  334, 6554,  ...,    0,    0,    0]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_tokenized['attention_mask'][0]"
      ],
      "metadata": {
        "id": "SO8mEnD3qybN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, questions, starts, ends, attention_masks):\n",
        "    self.questions = questions\n",
        "    self.starts = starts\n",
        "    self.ends = ends\n",
        "    self.attention_masks = attention_masks\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.questions)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    question = self.questions[item]\n",
        "    attention_mask = self.attention_masks[item]\n",
        "    start = self.starts[item]\n",
        "    end = self.ends[item]\n",
        "\n",
        "\n",
        "    return {\n",
        "      'input_ids': torch.tensor(question, dtype = torch.long),\n",
        "      'attention_mask': torch.tensor(attention_mask, dtype = torch.long),\n",
        "      'start_positions': torch.tensor(start, dtype=torch.long),\n",
        "      'end_positions' : torch.tensor(end, dtype = torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "7Keu6AN65RX0"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    questions = df_train_tokenized['input_ids'],\n",
        "    starts = df_train_tokenized['start_positions'],\n",
        "    ends = df_train_tokenized['end_positions'],\n",
        "    attention_masks = df_train_tokenized['attention_mask'],\n",
        ")\n",
        "\n",
        "val_dataset = TextDataset(\n",
        "    questions = df_val_tokenized['input_ids'],\n",
        "    starts = df_val_tokenized['start_positions'],\n",
        "    ends = df_val_tokenized['end_positions'],\n",
        "    attention_masks = df_val_tokenized['attention_mask'],\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(\n",
        "    questions = df_test_tokenized['input_ids'],\n",
        "    starts = df_test_tokenized['start_positions'],\n",
        "    ends = df_test_tokenized['end_positions'],\n",
        "    attention_masks = df_test_tokenized['attention_mask'],\n",
        ")"
      ],
      "metadata": {
        "id": "UOioYhl3HD8A"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data = TensorDataset(df_train_tokenized['input_ids'], df_train_tokenized['attention_mask'], df_train_tokenized['start_positions'], df_train_tokenized['end_positions'])\n",
        "val_data = TensorDataset(df_val_tokenized['input_ids'], df_val_tokenized['attention_mask'], df_val_tokenized['start_positions'], df_val_tokenized['end_positions'])\n",
        "test_data = TensorDataset(df_test_tokenized['input_ids'], df_test_tokenized['attention_mask'], df_test_tokenized['start_positions'], df_test_tokenized['end_positions'])"
      ],
      "metadata": {
        "id": "Rx_tAeR416Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "\n",
        "# train_sampler = RandomSampler(train_data)\n",
        "# val_sampler = RandomSampler(val_data)\n",
        "# test_sampler = RandomSampler(test_data)\n",
        "\n",
        "# train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
        "# val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)\n",
        "# test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "TRyz7LkCpOen"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import random\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "SEED = 19\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == torch.device(\"cuda\"):\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "Dowik7NKr89L"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('ixa-ehu/SciBERT-SQuAD-QuAC').to(device)\n",
        "model_checkpoint = 'ixa-ehu/SciBERT-SQuAD-QuAC'\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    output_dir = \"./logs/model_name\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    logging_dir = \"./logs/runs\",\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps = 25,\n",
        "    eval_steps = 250\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "Ijuz53S3ugwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101f0014-a7a4-426c-f2a8-4c6091515401"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/ixa-ehu/SciBERT-SQuAD-QuAC/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a8c52b63ffbb5c867d6270ae21e7905d08e9801af48232ff0e08e3b755da233b.7c3af56d16d03847a339f67a41d8e0c1108d55c0977344615e38b438ec54680d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"ixa-ehu/SciBERT-SQuAD-QuAC\",\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/ixa-ehu/SciBERT-SQuAD-QuAC/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/be1f037bb6f580098db99a74b56f047197796f8bfe208adedc38535a3375b3b4.f2bbdc825305b85292391508b969d6ba7727d7f6a6dad6303eddbd1e7c918e80\n",
            "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
            "\n",
            "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ixa-ehu/SciBERT-SQuAD-QuAC.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset= train_dataset,\n",
        "    eval_dataset= val_dataset,\n",
        "    data_collator=data_collator \n",
        ")"
      ],
      "metadata": {
        "id": "guDJ_GU8v1IR"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lEle2e9gyZEx",
        "outputId": "67c7ae01-9c54-4bf1-97f3-43799fd45165"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 8180\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5120\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5120' max='5120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5120/5120 1:15:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.620900</td>\n",
              "      <td>2.609655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.402900</td>\n",
              "      <td>2.576759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.760700</td>\n",
              "      <td>2.687246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.665200</td>\n",
              "      <td>2.931360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.239800</td>\n",
              "      <td>3.251777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.157200</td>\n",
              "      <td>3.443164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.924300</td>\n",
              "      <td>3.806898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.772000</td>\n",
              "      <td>4.105575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.618200</td>\n",
              "      <td>4.334773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.511800</td>\n",
              "      <td>4.463399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./logs/model_name/checkpoint-500\n",
            "Configuration saved in ./logs/model_name/checkpoint-500/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-1000\n",
            "Configuration saved in ./logs/model_name/checkpoint-1000/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-1500\n",
            "Configuration saved in ./logs/model_name/checkpoint-1500/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-1500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-2000\n",
            "Configuration saved in ./logs/model_name/checkpoint-2000/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-2000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-2500\n",
            "Configuration saved in ./logs/model_name/checkpoint-2500/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-2500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-3000\n",
            "Configuration saved in ./logs/model_name/checkpoint-3000/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-3500\n",
            "Configuration saved in ./logs/model_name/checkpoint-3500/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-3500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-4000\n",
            "Configuration saved in ./logs/model_name/checkpoint-4000/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-4000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-4500\n",
            "Configuration saved in ./logs/model_name/checkpoint-4500/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-4500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./logs/model_name/checkpoint-5000\n",
            "Configuration saved in ./logs/model_name/checkpoint-5000/config.json\n",
            "Model weights saved in ./logs/model_name/checkpoint-5000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 975\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1h 14min 3s, sys: 1min, total: 1h 15min 4s\n",
            "Wall time: 1h 15min 2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5120, training_loss=1.3621363122016192, metrics={'train_runtime': 4502.8351, 'train_samples_per_second': 18.166, 'train_steps_per_second': 1.137, 'total_flos': 2.13740747010048e+16, 'train_loss': 1.3621363122016192, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"test-baseline-trained\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5qG7DT6yrfQ",
        "outputId": "251b5a5c-920c-43df-dc5f-a0ed9f399b3f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to test-baseline-trained\n",
            "Configuration saved in test-baseline-trained/config.json\n",
            "Model weights saved in test-baseline-trained/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "with torch.no_grad():\n",
        "    output = trainer.model(**batch)\n",
        "output.keys()"
      ],
      "metadata": {
        "id": "syxU77SQzsDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a97e213-b99e-492c-c398-1c82b4cf6b02"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['loss', 'start_logits', 'end_logits'])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.start_logits.shape, output.end_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x42j53h2bNG2",
        "outputId": "69f7f274-536b-48f5-cab3-b4297925e562"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 512]), torch.Size([16, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P0NT8Q6bOya",
        "outputId": "354aa39f-e0bb-4c68-9d5f-32a7114b655e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([470, 224,   0, 430, 186,   0, 397, 154,   0, 412, 167,   0, 384, 140,\n",
              "           0, 458], device='cuda:0'),\n",
              " tensor([478, 232,   0, 438, 194,   0, 398, 155,   0, 414, 169,   0, 388, 144,\n",
              "           0, 459], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_best_size = 20\n",
        "import numpy as np\n",
        "\n",
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        if start_index <= end_index: \n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": \"\" \n",
        "                }\n",
        "            )"
      ],
      "metadata": {
        "id": "97BQVsHabQOR"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UNraEueUbVNd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}