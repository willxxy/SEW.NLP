ImageNet| Who funded the dataset?| WD is supported by Gordon Wu fellowship. RS is supported by the ERP and Upton fellowships. KL is funded by NSF grant CNS-0509447 and by research grants from Google, Intel, Microsoft and Yahoo!. LFF is funded by research grants from Microsoft and Google
ImageNet| What does the dataset represent?| tens of millions of annotated images organized by the semantic hierarchy of WordNet
ImageNet| How was data collected?| To collect a highly accurate dataset, we rely on humans to verify each candidate image collected in the previous step for a given synset. This is achieved by using the service of Amazon Mechanical Turk (AMT), an online platform on which one can put up tasks for users to complete and to get paid. AMT has been used for labeling vision data
ImageNet| For what purpose was the dataset created?| A benchmark dataset
Hierarchical sales data| What does the dataset represent?| The dataset consists of 118 daily time series representing the demand for pasta from 01/01/ 2014 to 31/12/2018. Besides univariate time series data, the quantity sold is integrated by information on the presence or the absence of a promotion (no detail on the type of promotion on the final price is available)
Hierarchical sales data| Was any preprocessing performed?| for each series, as explanatory variables, we add a binary variable representing the presence of promotion if the disaggregation is computed at the item level or a variable representing the relative number of items in promotion for each brand if the disaggregation is computed at the brand level. In both cases, dummy variables representing the day of the week and the month are also added to the model. As for the number of lagged observations of the aggregate demand, we consider time windows of length w = 30 days with a hop size of 1 day
Hierarchical sales data| What is the baseline metric of the dataset?| average MASE
Smartphone dataset| Was any preprocessing performed?| We extracted 14 different features from the continuous acceleration and discrete motion data. These features were calculated in time and frequency domains as listed in the Table I. We applied a short sliding windows (1 second, 50% overlap) to allow a prompt response and to improve the detection of anomaly based on small collective movements
Smartphone dataset| For what purpose was the dataset created?| We also present our dataset for public research as there is no such dataset available to perform experiments on crowds for detecting unusual behaviours
Palmer penguins| How was data collected?| Field research was conducted on Pygoscelis penguins nesting on several islands within the Palmer Archipelago west of the AP near Anvers Island (64u469S, 64u039W, Fig. 1a-c), during the austral summers of 2007/08, 2008/09, and 2009/10
Sudanese Twitter Dataset| Was any preprocessing performed?| The preprocessing includes case folding, stopwords removal, stemming, tokenizing, and text representation. Prior to classification, for the feature generation, we utilize term frequency-inverse document frequency (TFIDF).
Sudanese Twitter Dataset| How was data collected?| We gathered dataset from Twitter API between January and March 2019 with 2518 tweets in total.
Sudanese Twitter Dataset| For what purpose was the dataset collected?| Sundanese is the second-largest tribe in Indonesia which possesses many dialects. This condition has gained attention for many researchers to analyze emotion especially on social media. However, with barely available Sundanese dataset, this condition makes understanding sundanese emotion is a challenging task
Open Web Text Corpus| Was any preprocessing performed?| We removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks.
Open Web Text Corpus| How was data collected?| we created a new web scrape which emphasizes document quality. To do this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, which received at least 3 karma.
Open Web Text Corpus| For what purpose was the dataset created?| building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible.
Auction Verification| What does the dataset represent?| Each row corresponds to verifying one property against the underlying Petri Net.
Auction Verification| For what purpose was the dataset created?| This paper presents an approach to analyze and predict verification results in a data-driven manner. To this end, we create a dataset by verifying properties of an SMR auction model.
Auction Verification| Was any preprocessing performed?| we use approaches from related work to reduce the state space on the level of process models (Activity reduce process, Section IV-B). Then, we map the reduced BPMN model to Petri Nets [43] (Activity transform to Petri Nets, Section IV-C). Given the final Petri Nets and properties in form of CTL formulas [44], we employ an off-the-shelf model checker to verify properties
LT-FS-ID: Intrusion detection in WSNs| For what purpose was the dataset created?| This study proposed a novel approach to estimate the number of barriers required for intrusion detection.
LT-FS-ID: Intrusion detection in WSNs| How was data collected?| we extracted the datasets synthetically through simulations. To do so, we consider a finite number of sensors (N), distributed uniformly and randomly in a rectangular RoI
LT-FS-ID: Intrusion detection in WSNs| Which metric was used to evaluate the models?| we evaluated its performance on the testing datasets using R, RMSE, MSE, bias, and computational time complexity as performance metrics
NATICUSdroid (Android Permissions) Dataset| How was the dataset collected?| We analyze declared permissions in more than 29,000 benign and malware collected during 2010-2019 to identify the most significant permissions based on the trend. Subsequently, we collect these identified permissions that include both the native and custom permissions.
NATICUSdroid (Android Permissions) Dataset| For what purpose was the dataset created?| With the constantly changing Android environment, there is a need to build a malware detection architecture using more recent and robust data, considering the most critical permissions capable of evolving
NATICUSdroid (Android Permissions) Dataset| Was any preprocessing performed?| First, we computed the frequency of each permission in all the apps of the dataset, i.e., how many apps have declared specific permission in their manifest files. This gave us an insight on which permissions are used more frequently than others, and could potentially work against the model. Second, we applied backward elimination to remove features that were not statistically significant in the classification process. Although this process reduces the number of required features and improves accuracy, it does not take care of the multicollinearity problem in the dataset that holds back the classification accuracy. Therefore, we performed a collinearity check by finding correlations among all possible pairs of features. After this check, we kept only one feature from the strongly correlated feature pairs.


GLUE| How was the data collected?| None of the datasets in GLUE were created from scratch for the benchmark; we rely on preexisting datasets because they have been implicitly agreed upon by the NLP community as challenging and interesting.
GLUE| For what purpose was the dataset collected?| If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a more unified model that can learn to execute a range of different linguistic tasks in different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE) benchmark
SQuAD| What does the dataset represent?| 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage
SQuAD| For what purpose was the dataset collected?| To address the need for a large and high-quality reading comprehension dataset
SQuAD| Who funded the data collection?| We would like to thank Durim Morina and Professor Michael Bernstein for their help in crowdsourcing the collection of our dataset, both in terms of funding and technical support of the Daemo platform.
Penn Treebank| What does the dataset represent?| a corpus 1 consisting of over 4.5 million words of American English
CBT| For what purpose was the dataset created?| to test the role of memory and context in language processing and understanding.
CBT| How was data collected?| built from books that are freely available thanks to Project Gutenberg
MIMIC-III| What does the dataset represent?| Data available in the MIMIC-III database ranges from time-stamped, nurse-verified physiological measurements made at the bedside to free-text interpretations of imaging studies provided by the radiology department.
MIMIC-III| Who funded the dataset?| This research and development was supported by grants NIH-R01-EB017205, NIH-R01-EB001659, and NIH-R01-GM104987 from the National Institutes of Health
CheXpert| What does the dataset represent?|  The dataset consists of 224,316 chest radiographs of 65,240 patients labeled for the presence of 14 common chest radiographic observations.
CheXpert| What task can be performed on the dataset?| The CheXpert task is to predict the probability of 14 different observations from multi-view chest radiographs 
CheXpert| For what purpose was the dataset created?|  The CheXpert dataset that we introduce features radiologist-labeled validation and test sets which serve as strong reference standards, as well as expert scores to allow for robust evaluation of different algorithms.
ChestX-ray| For what purpose was the dataset created?| We attempt to build a "machine-human annotated" comprehensive chest X-ray database that presents the realistic clinical and methodological challenges of handling at least tens of thousands of patients 
ChestX-ray| Who funded the dataset?| This work was supported by the Intramural Research Programs of the NIH Clinical Center and National Library of Medicine. We thank NVIDIA Corporation for the GPU donation.
CORD-19| For what purpose was the dataset created?| CORD-19 aims to connect the machine learning community with biomedical domain experts and policy makers in the race to identify effective treatments and management policies for COVID-19
CORD-19| Was any preprocessing performed?| We perform the following operations to harmonize and deduplicate all metadata:1. Cluster papers using paper identifiers 2. Select canonical metadata for each cluster 3. Filter clusters to remove unwanted entries
CORD-19| What does the dataset represent?| CORD-19 has grown rapidly, now consisting of over 140K papers with over 72K full texts. Over 47K papers and 7K preprints on COVID-19 and coronaviruses have been released since the start of 2020, comprising nearly 40% of papers in the dataset.
CORD-19| Who funded the dataset?| This work was supported in part by NSF Convergence Accelerator award 1936940, ONR grant N00014-18-1-2193, and the University of Washington WRF/Cable Professorship
COCO| Fow what purpose was the data collected?| advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding.
COCO| How was the data collected?|  we employed a novel pipeline for gathering data with extensive use of Amazon Mechanical Turk. First and most importantly, we harvested a large set of images containing contextual relationships and noniconic object views. We accomplished this using a surprisingly simple yet effective technique that queries for pairs of objects in conjunction with images retrieved via scene-based queries [17], [3]. Next, each image was labeled as containing particular object categories using a hierarchical labeling approach [18]. For each category found, the individual instances were labeled, verified, and finally segmented
COCO| What does the dataset represent?|  contains 91 common object categories with 82 of them having more than 5,000 labeled instances, Fig. 6. In total the dataset has 2,500,000 labeled instances in 328,000 images. 
COCO| Who funded the dataset?| Funding for all crowd worker tasks was provided by Microsoft. P.P. and D.R. were supported by ONR MURI Grant N00014-10-1-0933.
DAVIS| For what purpose was the data collected?| a standardized and widely adopted evaluation methodology for video object segmentation does not yet exists. To this end, we introduce a new dataset specifically designed for the task of video object segmentation. 
DAVIS| What does the dataset represent?| The dataset, which will be made publicly available, contains fifty densely and professionally annotated high-resolution Full HD video sequences, with pixel-accurate ground-truth data provided for every video frame.
DAVIS| Who funded the creation of the dataset?| This work was partially funded by an SNF award (200021 143598).
Waymo Open Dataset| For what purpose was the data collected?| In an effort to help align the research community's contributions with real-world selfdriving problems, we introduce a new large-scale, high quality, diverse dataset
Waymo Open Dataset| What does the dataset represent?|  Our dataset contains around 12 million LiDAR box annotations and around 12 million camera box annotations, giving rise to around 113k LiDAR object tracks and around 250k camera image tracks.
Waymo Open Dataset| How was the data collected?| The data collection was conducted using five LiDAR sensors and five high-resolution pinhole cameras. We restrict the range of the LiDAR data, and provide data for the first two returns of each laser pulse.
MPII| For what purpose was the dataset created?| In this paper we introduce a novel benchmark "MPII Human Pose" 1 that makes a significant advance in terms of diversity and difficulty, a contribution that we feel is required for future developments in human body models.
MPII| How was the data collected?| As a first step of the data collection we manually query YouTube using descriptions of activities from [1]. We select up to 10 videos for each activity filtering out videos of low quality and those that do not include people. This resulted in 3, 913 videos spanning 491 different activities 
MPII| Was any preprocessing performed?| we manually pick several frames with people from each video. As the focus of our benchmark is pose estimation we do not include video frames in which people are severely truncated or in which pose is not recognizable due to poor image quality or small scale. We aim to select frames that either depict different people present in the video or the same person in a substantially different pose. In addition we restrict the selected frames to be at least 5 seconds apart
MPII| What does the dataset represent?| In this paper we introduce a large dataset of images that covers a wide variety of human poses and clothing types and includes people interacting with various objects and environments.
MLQA| For what purpose was the dataset created?| A purpose-built evaluation benchmark dataset covering a range of diverse languages, and following the popular extractive QA paradigm on a practically-useful domain would be a powerful testbed for cross-lingual QA models. With this work, we present such a benchmark, MLQA, and hope that it serves as an accelerator for multilingual QA in the way datasets such as SQuAD (Rajpurkar et al., 2016) have done for its monolingual counterpart.
MLQA| What does the dataset represent?| The resulting corpus has between 5,000 and 6,000 instances in each language, and more than 12,000 in English. Each instance has an aligned equivalent in multiple other languages (always including English), the majority being 4-way aligned. Combined, there are over 46,000 QA annotations.
OLID| Who funded the dataset creation?| This research presented was partially supported by an ERAS fellowship awarded to Marcos Zampieri by the University of Wolverhampton.
Conceptual Captions| Was any preprocessing performed?| The following steps are applied to achieve text transformations:• noun modifiers of certain types (proper nouns, numbers, units) are removed;• dates, durations, and preposition-based locations (e.g., "in Los Angeles") are removed;• named-entities are identified, matched against the KG entries, and substitute with their hypernym;• resulting coordination noun-phrases with the same head (e.g., "actor and actor") are resolved into a single-head, pluralized form (e.g., "actors")
HotpotQA| For what purpose was the dataset created?| we aim at creating a QA dataset that requires reasoning over multiple documents, and does so in natural language, without constraining itself to an existing knowledge base or knowledge schema. We also want it to provide the system with strong supervision about what text the answer is actually derived from, to help guide systems to perform meaningful and explainable reasoning.
HotpotQA| Who funded the dataset creation?| This work is partly funded by the Facebook ParlAI Research Award. ZY, WWC, and RS are supported by a Google grant, the DARPA grant D17AP00001, the ONR grants N000141512791, N000141812861, and the Nvidia NVAIL Award. SZ and YB are supported by Mila, Université de Montréal. PQ and CDM are supported by the National Science Foundation under Grant No. IIS-1514268. 
HotpotQA| Was any preprocessing performed?| We downloaded the dump of English Wikipedia of October 1, 2017, and extracted text and hyperlinks with WikiExtractor.8 We use Stanford CoreNLP 3.8.0 (Manning et al., 2014) for word and sentence tokenization. We use the resulting sentence boundaries for collection of supporting facts, and use token boundaries to check whether Turkers are providing answers that cover spans of entire tokens to avoid nonsensical partial-word answers.
HotpotQA| What does the dataset represent?| We introduce HOTPOTQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison.